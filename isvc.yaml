apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: bloom-560m
  annotations:
    identities.bloomberg.com/bcs: dsplatform-bcs-gen
spec:
  predictor:
    model:
      modelFormat:
        name: "triton"
        version: "2"
      runtime: tritonserver-ft
      storageUri: s3://platform/kserve/bloom-a100
      resources:
        limits: 
          cpu: "2"
          memory: 8Gi
          nvidia.com/gpu: "2"
        requests: 
          cpu: "1"
          memory: 4Gi
          nvidia.com/gpu: "2"
  transformer:
    containers:
      - image: artprod.dev.bloomberg.com/dspuser/s-dsplatform/bloom-560m-example-transformer:latest
        name: kserve-container
        command: 
          - python
          - transformer.py
        args: 
          - --model_name
          - fastertransformer
          - --protocol
          - grpc-v2
          - --tokenizer_path
          - /mnt/models/
        env:
          - name: STORAGE_URI
            value: s3://platform/kserve/bloom-a100/fastertransformer/1/tokenizer
